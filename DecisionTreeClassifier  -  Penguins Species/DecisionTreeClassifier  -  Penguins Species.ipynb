{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score\n",
    "\n",
    "def input_mlflow(cv_r_v,increment_number,X_train, y_train,pipeline,run_name,experiment_id,X_test,y_test):\n",
    "    \n",
    "\n",
    "    for wi in range(len(cv_r_v)):\n",
    "        \n",
    "        #====================skema===================\n",
    "        cols_spec = []\n",
    "        data_map = {\n",
    "                'int64': 'integer',\n",
    "                'float64': 'double',\n",
    "                'bool': 'boolean',\n",
    "                'str': 'string',\n",
    "                'object': 'string',\n",
    "                \"date\": 'datetime'\n",
    "            }\n",
    "\n",
    "        for name, dtype in X_train.dtypes.to_dict().items():\n",
    "            cols_spec.append(ColSpec(name=name, type=data_map[str(dtype)]))\n",
    "        input_schema = Schema(inputs=cols_spec)\n",
    "        output_schema = Schema([ColSpec(name=\"label\", type=\"string\")])\n",
    "        #parameter = ParamSpec(name=\"model_name\", dtype=\"string\", default=\"model1\")\n",
    "        #param_schema = ParamSchema(params=[parameter])\n",
    "        model_signature = ModelSignature(inputs=input_schema, outputs=output_schema)#, params=param_schema)\n",
    "        #print(\"MODEL SIGNATURE\")\n",
    "        #print(model_signature.to_dict())\n",
    "\n",
    "        model_signature = infer_signature(X_train, y_train)#, params={\"model_name\": \"model1\"})\n",
    "        #print(\"MODEL SIGNATURE\")\n",
    "        #print(model_signature.to_dict())\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        #====================buat run baru===================\n",
    "        run_name_with_increment = f\"{run_name}__{increment_number}\"\n",
    "            # Membuka run MLflow\n",
    "        with mlflow.start_run(run_name=run_name_with_increment, experiment_id=experiment_id) as run:\n",
    "            # Mendapatkan run_id\n",
    "            run_id = mlflow.active_run().info.run_id\n",
    "        \n",
    "        print(f\"mlruns/{experiment_id}/{run_id}/artifacts/grid_search__{increment_number}\")\n",
    "        increment_number=increment_number+1\n",
    "        \n",
    "        \n",
    "        #====================buat parameter model dan metric===================\n",
    "\n",
    "        # Definisikan grid parameter untuk dicari\n",
    "        param_grid = cv_r_v[wi]\n",
    "        \n",
    "        #====================model===================\n",
    "\n",
    "        # Inisialisasi GridSearchCV\n",
    "        grid_search = GridSearchCV(pipeline, param_grid, cv=5,  scoring='accuracy')\n",
    "\n",
    "        # Lakukan pencarian grid\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        \n",
    "            \n",
    "        #====================parameter===================\n",
    "        pipe=grid_search.best_estimator_\n",
    "        \n",
    "        \n",
    "        first_step_name = list(pipe.named_steps.keys())[0:len(pipe)]\n",
    "        \n",
    "\n",
    "        for i in range(len(pipe)):\n",
    "            # Mendapatkan parameter dari langkah 'sca'\n",
    "            nama=first_step_name[i]\n",
    "            sca_params = pipe.get_params()[nama]\n",
    "\n",
    "                # Membuka run MLflow\n",
    "            with mlflow.start_run( experiment_id=experiment_id,run_id=run_id) as run:\n",
    "\n",
    "                # Log parameter secara otomatis menggunakan loop\n",
    "                for param_name, param_value in sca_params.get_params().items():\n",
    "                    param_name=nama+'__'+param_name\n",
    "                    #print(param_name,param_value)\n",
    "                    mlflow.log_param(param_name, param_value)\n",
    "                    \n",
    "                    \n",
    "                #====================metric===================\n",
    "                #matric\n",
    "                hasil_test=grid_search.predict(X_test)\n",
    " \n",
    "                \n",
    "                \n",
    "                \n",
    "                                # Menghitung akurasi\n",
    "                accuracy = accuracy_score(y_test, hasil_test)\n",
    "\n",
    "                # Menghitung F1 score\n",
    "                f1 = f1_score(y_test, hasil_test,average='macro')\n",
    "\n",
    "                # Menghitung recall\n",
    "                recall = recall_score(y_test, hasil_test,average='macro')\n",
    "\n",
    "                \n",
    "                \n",
    "                                # Menghitung precision\n",
    "                precision = precision_score(y_test, hasil_test,average='macro')\n",
    "\n",
    "\n",
    "   \n",
    "                \n",
    "\n",
    "                # log model \n",
    "                mlflow.sklearn.log_model(sk_model=grid_search, artifact_path=\"grid_search__\"+str(increment_number-1),signature=model_signature)\n",
    "\n",
    "                metrics = {\n",
    "                    'mean_test_score':pd.DataFrame(grid_search.cv_results_)['mean_test_score'].values[0],\n",
    "                    \"accuracy\": accuracy,\n",
    "                    \"f1\": f1,\n",
    "                    \"recall\": recall,\n",
    "                    \"precision\": precision,\n",
    "         \n",
    "                }\n",
    "\n",
    "                mlflow.log_metrics(metrics)\n",
    "                \n",
    "                \n",
    "        mlflow.end_run()\n",
    "    print('selesai')\n",
    "\n",
    "        \n",
    "    return increment_number\n",
    "\n",
    "\n",
    "     \n",
    "\n",
    "def ambil_best(grid_search,n):\n",
    "    cv_r=grid_search.copy()\n",
    "    cv_r_v=cv_r.sort_values(by=['mean_test_score', 'std_test_score'], ascending=[False, True])\n",
    "    cv_r_v=cv_r_v.head(n)['params'].values\n",
    "\n",
    "    for wi in range(len(cv_r_v)):\n",
    "\n",
    "        for i in list(cv_r_v[wi]):\n",
    "\n",
    "            cv_r_v[wi][i]=[cv_r_v[wi][i]]\n",
    "\n",
    "\n",
    "    return cv_r_v\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow.sklearn\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "\n",
    "def test(model_paths,X_test2,y_test):\n",
    "    ii=1\n",
    "    for i, model_path in enumerate(model_paths, start=1):\n",
    "        print(f'\\n\\n================================| model {ii} |==========================================')\n",
    "        \n",
    "        model = mlflow.sklearn.load_model(model_path)\n",
    "        y_pred = model.predict(X_test2)\n",
    "        print(f\"Classification Report for model {model_path[-2:]}:\")\n",
    "        print(classification_report(y_test, y_pred))\n",
    "        print(f\"Confusion Matrix for model {model_path[-2:]}:\\n\", confusion_matrix(y_test, y_pred), '\\n\\n')\n",
    "        \n",
    "        ii=ii+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score\n",
    "# import libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "from scipy.stats import shapiro,normaltest,kstest,jarque_bera\n",
    "import pingouin as pg\n",
    "from pingouin import kruskal\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import mlflow\n",
    "\n",
    "\n",
    "\n",
    "import mlflow\n",
    "\n",
    "from mlflow.models.signature import ModelSignature\n",
    "from mlflow.models.signature import infer_signature\n",
    "from mlflow.types.schema import Schema\n",
    "from mlflow.types.schema import ParamSchema\n",
    "from mlflow.types.schema import ParamSpec\n",
    "from mlflow.types.schema import ColSpec\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "import pandas as pd\n",
    "from typing import Tuple\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error,mean_squared_error, r2_score\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import make_column_selector as selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2_contingency, fisher_exact"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prepare data 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/data_csv/data_bersih.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>species</th>\n",
       "      <th>island</th>\n",
       "      <th>culmen_length_mm</th>\n",
       "      <th>culmen_depth_mm</th>\n",
       "      <th>flipper_length_mm</th>\n",
       "      <th>body_mass_g</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Adelie</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>39.10</td>\n",
       "      <td>18.7</td>\n",
       "      <td>181.0</td>\n",
       "      <td>3750.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Adelie</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>39.50</td>\n",
       "      <td>17.4</td>\n",
       "      <td>186.0</td>\n",
       "      <td>3800.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Adelie</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>40.30</td>\n",
       "      <td>18.0</td>\n",
       "      <td>195.0</td>\n",
       "      <td>3250.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Adelie</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>44.45</td>\n",
       "      <td>17.3</td>\n",
       "      <td>197.0</td>\n",
       "      <td>4050.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Adelie</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>36.70</td>\n",
       "      <td>19.3</td>\n",
       "      <td>193.0</td>\n",
       "      <td>3450.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  species     island  culmen_length_mm  culmen_depth_mm  flipper_length_mm  \\\n",
       "0  Adelie  Torgersen             39.10             18.7              181.0   \n",
       "1  Adelie  Torgersen             39.50             17.4              186.0   \n",
       "2  Adelie  Torgersen             40.30             18.0              195.0   \n",
       "3  Adelie  Torgersen             44.45             17.3              197.0   \n",
       "4  Adelie  Torgersen             36.70             19.3              193.0   \n",
       "\n",
       "   body_mass_g  \n",
       "0       3750.0  \n",
       "1       3800.0  \n",
       "2       3250.0  \n",
       "3       4050.0  \n",
       "4       3450.0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "species              0\n",
       "island               0\n",
       "culmen_length_mm     0\n",
       "culmen_depth_mm      0\n",
       "flipper_length_mm    0\n",
       "body_mass_g          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 343 entries, 0 to 342\n",
      "Data columns (total 6 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   species            343 non-null    object \n",
      " 1   island             343 non-null    object \n",
      " 2   culmen_length_mm   343 non-null    float64\n",
      " 3   culmen_depth_mm    343 non-null    float64\n",
      " 4   flipper_length_mm  343 non-null    float64\n",
      " 5   body_mass_g        343 non-null    float64\n",
      "dtypes: float64(4), object(2)\n",
      "memory usage: 16.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df.drop(columns=['species'])\n",
    "y=df['species']\n",
    "\n",
    "Xs=pd.get_dummies(X,drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(Xs,y, test_size=0.15, random_state=142)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "model = DecisionTreeClassifier(random_state=101)\n",
    "model.fit(X_train,y_train)\n",
    "model.get_depth()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>island</th>\n",
       "      <th>culmen_length_mm</th>\n",
       "      <th>culmen_depth_mm</th>\n",
       "      <th>flipper_length_mm</th>\n",
       "      <th>body_mass_g</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>Dream</td>\n",
       "      <td>51.0</td>\n",
       "      <td>18.8</td>\n",
       "      <td>203.0</td>\n",
       "      <td>4100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>Biscoe</td>\n",
       "      <td>49.1</td>\n",
       "      <td>14.8</td>\n",
       "      <td>220.0</td>\n",
       "      <td>5150.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>Biscoe</td>\n",
       "      <td>37.6</td>\n",
       "      <td>17.0</td>\n",
       "      <td>185.0</td>\n",
       "      <td>3600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>Dream</td>\n",
       "      <td>37.8</td>\n",
       "      <td>18.1</td>\n",
       "      <td>193.0</td>\n",
       "      <td>3750.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>Biscoe</td>\n",
       "      <td>34.5</td>\n",
       "      <td>18.1</td>\n",
       "      <td>187.0</td>\n",
       "      <td>2900.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     island  culmen_length_mm  culmen_depth_mm  flipper_length_mm  body_mass_g\n",
       "185   Dream              51.0             18.8              203.0       4100.0\n",
       "254  Biscoe              49.1             14.8              220.0       5150.0\n",
       "62   Biscoe              37.6             17.0              185.0       3600.0\n",
       "149   Dream              37.8             18.1              193.0       3750.0\n",
       "54   Biscoe              34.5             18.1              187.0       2900.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.20, random_state=142)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# make pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "numeric_features = selector(dtype_include='float64')\n",
    "categorical_features = selector(dtype_include='object')\n",
    "\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(drop='first', handle_unknown='error'))\n",
    "])\n",
    "\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('dt', DecisionTreeClassifier(random_state=101))])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# make param_grid "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'dt__max_depth': list(range(2, 7)),\n",
    "    'dt__criterion': ['gini', 'entropy'],\n",
    "    'dt__min_samples_split': [2, 5, 10],\n",
    "    'dt__min_samples_leaf': [1, 2, 4],\n",
    "    'dt__max_features': [None, 'auto', 'sqrt', 'log2'],\n",
    "    'dt__max_leaf_nodes': [None, 10, 20, 30],\n",
    "    'dt__min_impurity_decrease': [0.0, 0.01, 0.1]\n",
    "}\n",
    "\n",
    "grid_search_dt = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy')\n",
    "grid_search_dt.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = pd.DataFrame(grid_search_dt.cv_results_)\n",
    "df_1.drop_duplicates(subset=['mean_test_score', 'std_test_score', 'rank_test_score'], inplace=True)\n",
    "df_1 = df_1.sort_values(by=['mean_test_score', 'std_test_score'], ascending=[False, True])\n",
    "df_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "data_ambil=ambil_best(df_1,10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ambil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    experiment_id = mlflow.create_experiment(\n",
    "        name=\"project_DecisionTreeClassifier_new\",\n",
    "        tags={\"env\": \"dev\", \"version\": \"1.0.0\"},\n",
    "    )\n",
    "\n",
    "    print(experiment_id)\n",
    "    \n",
    "\n",
    "mlflow.end_run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_name = \"log___\"\n",
    "increment_number = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mlruns/776004161425800329/b90db1c4dafb4b65ab48c49cba21b830/artifacts/grid_search__1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ACER\\anaconda3\\lib\\site-packages\\_distutils_hack\\__init__.py:30: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mlruns/776004161425800329/24a59867967848528ca40fdb6991f463/artifacts/grid_search__2\n",
      "mlruns/776004161425800329/953b3bcf771f4c57ac6bad253a39a2b6/artifacts/grid_search__3\n",
      "mlruns/776004161425800329/f83180406a0946d5bb421f5776adfb2e/artifacts/grid_search__4\n",
      "mlruns/776004161425800329/810d77ac339b4990b153414527a27816/artifacts/grid_search__5\n",
      "mlruns/776004161425800329/eff4ea110e5340bc9c0aeafe0cb86d9a/artifacts/grid_search__6\n",
      "mlruns/776004161425800329/bb255a61228a4da9b259f41d2c42580d/artifacts/grid_search__7\n",
      "mlruns/776004161425800329/2ce4bcc555c940e4b0d1651ae92be233/artifacts/grid_search__8\n",
      "mlruns/776004161425800329/d686423b105a4cd598c668d26f119cb6/artifacts/grid_search__9\n",
      "mlruns/776004161425800329/8480e85685e045149d778fb76524d7f3/artifacts/grid_search__10\n",
      "selesai\n"
     ]
    }
   ],
   "source": [
    "\n",
    "increment_number=input_mlflow(data_ambil,increment_number,X_train, y_train,pipeline,run_name,experiment_id,X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "================================| model 1 |==========================================\n",
      "Classification Report for model _1:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Adelie       0.97      0.97      0.97        32\n",
      "   Chinstrap       0.93      1.00      0.96        13\n",
      "      Gentoo       1.00      0.96      0.98        24\n",
      "\n",
      "    accuracy                           0.97        69\n",
      "   macro avg       0.97      0.98      0.97        69\n",
      "weighted avg       0.97      0.97      0.97        69\n",
      "\n",
      "Confusion Matrix for model _1:\n",
      " [[31  1  0]\n",
      " [ 0 13  0]\n",
      " [ 1  0 23]] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "================================| model 2 |==========================================\n",
      "Classification Report for model _2:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Adelie       0.89      0.97      0.93        32\n",
      "   Chinstrap       0.92      0.85      0.88        13\n",
      "      Gentoo       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.93        69\n",
      "   macro avg       0.93      0.91      0.92        69\n",
      "weighted avg       0.93      0.93      0.93        69\n",
      "\n",
      "Confusion Matrix for model _2:\n",
      " [[31  1  0]\n",
      " [ 2 11  0]\n",
      " [ 2  0 22]] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "================================| model 3 |==========================================\n",
      "Classification Report for model _3:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Adelie       0.97      0.97      0.97        32\n",
      "   Chinstrap       0.93      1.00      0.96        13\n",
      "      Gentoo       1.00      0.96      0.98        24\n",
      "\n",
      "    accuracy                           0.97        69\n",
      "   macro avg       0.97      0.98      0.97        69\n",
      "weighted avg       0.97      0.97      0.97        69\n",
      "\n",
      "Confusion Matrix for model _3:\n",
      " [[31  1  0]\n",
      " [ 0 13  0]\n",
      " [ 1  0 23]] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "================================| model 4 |==========================================\n",
      "Classification Report for model _4:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Adelie       0.91      0.97      0.94        32\n",
      "   Chinstrap       0.92      0.92      0.92        13\n",
      "      Gentoo       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.94        69\n",
      "   macro avg       0.94      0.94      0.94        69\n",
      "weighted avg       0.94      0.94      0.94        69\n",
      "\n",
      "Confusion Matrix for model _4:\n",
      " [[31  1  0]\n",
      " [ 1 12  0]\n",
      " [ 2  0 22]] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "================================| model 5 |==========================================\n",
      "Classification Report for model _5:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Adelie       0.94      0.97      0.95        32\n",
      "   Chinstrap       0.93      1.00      0.96        13\n",
      "      Gentoo       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.96        69\n",
      "   macro avg       0.96      0.96      0.96        69\n",
      "weighted avg       0.96      0.96      0.96        69\n",
      "\n",
      "Confusion Matrix for model _5:\n",
      " [[31  1  0]\n",
      " [ 0 13  0]\n",
      " [ 2  0 22]] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "================================| model 6 |==========================================\n",
      "Classification Report for model _6:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Adelie       0.88      0.94      0.91        32\n",
      "   Chinstrap       0.85      0.85      0.85        13\n",
      "      Gentoo       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.91        69\n",
      "   macro avg       0.91      0.90      0.90        69\n",
      "weighted avg       0.92      0.91      0.91        69\n",
      "\n",
      "Confusion Matrix for model _6:\n",
      " [[30  2  0]\n",
      " [ 2 11  0]\n",
      " [ 2  0 22]] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "================================| model 7 |==========================================\n",
      "Classification Report for model _7:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Adelie       0.97      0.97      0.97        32\n",
      "   Chinstrap       0.93      1.00      0.96        13\n",
      "      Gentoo       1.00      0.96      0.98        24\n",
      "\n",
      "    accuracy                           0.97        69\n",
      "   macro avg       0.97      0.98      0.97        69\n",
      "weighted avg       0.97      0.97      0.97        69\n",
      "\n",
      "Confusion Matrix for model _7:\n",
      " [[31  1  0]\n",
      " [ 0 13  0]\n",
      " [ 1  0 23]] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "================================| model 8 |==========================================\n",
      "Classification Report for model _8:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Adelie       0.94      0.94      0.94        32\n",
      "   Chinstrap       0.87      1.00      0.93        13\n",
      "      Gentoo       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.94        69\n",
      "   macro avg       0.93      0.95      0.94        69\n",
      "weighted avg       0.95      0.94      0.94        69\n",
      "\n",
      "Confusion Matrix for model _8:\n",
      " [[30  2  0]\n",
      " [ 0 13  0]\n",
      " [ 2  0 22]] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "================================| model 9 |==========================================\n",
      "Classification Report for model _9:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Adelie       0.89      0.97      0.93        32\n",
      "   Chinstrap       0.92      0.85      0.88        13\n",
      "      Gentoo       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.93        69\n",
      "   macro avg       0.93      0.91      0.92        69\n",
      "weighted avg       0.93      0.93      0.93        69\n",
      "\n",
      "Confusion Matrix for model _9:\n",
      " [[31  1  0]\n",
      " [ 2 11  0]\n",
      " [ 2  0 22]] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "================================| model 10 |==========================================\n",
      "Classification Report for model 10:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Adelie       0.88      0.94      0.91        32\n",
      "   Chinstrap       0.85      0.85      0.85        13\n",
      "      Gentoo       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.91        69\n",
      "   macro avg       0.91      0.90      0.90        69\n",
      "weighted avg       0.92      0.91      0.91        69\n",
      "\n",
      "Confusion Matrix for model 10:\n",
      " [[30  2  0]\n",
      " [ 2 11  0]\n",
      " [ 2  0 22]] \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_paths = [\n",
    "  \n",
    "    'mlruns/776004161425800329/b90db1c4dafb4b65ab48c49cba21b830/artifacts/grid_search__1',\n",
    "    'mlruns/776004161425800329/24a59867967848528ca40fdb6991f463/artifacts/grid_search__2',\n",
    "    'mlruns/776004161425800329/953b3bcf771f4c57ac6bad253a39a2b6/artifacts/grid_search__3',\n",
    "    'mlruns/776004161425800329/f83180406a0946d5bb421f5776adfb2e/artifacts/grid_search__4',\n",
    "    'mlruns/776004161425800329/810d77ac339b4990b153414527a27816/artifacts/grid_search__5',\n",
    "    'mlruns/776004161425800329/eff4ea110e5340bc9c0aeafe0cb86d9a/artifacts/grid_search__6',\n",
    "    'mlruns/776004161425800329/bb255a61228a4da9b259f41d2c42580d/artifacts/grid_search__7',\n",
    "    'mlruns/776004161425800329/2ce4bcc555c940e4b0d1651ae92be233/artifacts/grid_search__8',\n",
    "    'mlruns/776004161425800329/d686423b105a4cd598c668d26f119cb6/artifacts/grid_search__9',\n",
    "    'mlruns/776004161425800329/8480e85685e045149d778fb76524d7f3/artifacts/grid_search__10'\n",
    "\n",
    "    ]\n",
    "\n",
    "test(model_paths,X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
